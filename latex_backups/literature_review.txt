\addcontentsline{toc}{chapter}{Literature review}
\chapter*{\hfill \huge{Literature Review}}
\addcontentsline{toc}{section}{Risk}
\section*{\large{Risk}}
One of the main aims of any investor is to minimise risks associated with their investments. Risk refers to the potential for financial loss or uncertainty in investment outcomes. It encompasses various factors that can impact the value of an investment, such as market fluctuations, economic conditions, and specific risks related to individual assets or companies. By understanding and managing different types of risk, investors can make informed decisions to protect their capital and optimise their investment returns. Risk can be categorised into two types.
\begin{enumerate}
    \item \textbf{Systematic Risk}: Systematic risk refers to undiversifiable market risk. It includes inflation, political instability, war, economic changes etc. Investors aim to manage this risk by choosing the best asset allocation, robust hedging strategies and making informed decisions based on the macroeconomic factors/indicators (\cite{jameschensystrisk}).
    \item \textbf{Unsystematic risk}: Unsystematic risk, also known as nonsystematic risk, refers to the risk associated with a specific company or an industry. In the context of a portfolio, unsystematic risk can be reduced by diversification of capital across several asset classes(\cite{jameschenunsystrisk}). 
\end{enumerate}
\addcontentsline{toc}{section}{Diversification and Optimisation}
\section*{\large{Diversification and Optimisation}}
Diversification, a fundamental concept in asset management, is used to manage unsystematic risk. A diversified portfolio comprises a mix of unique asset classes to reduce the exposure to a single security (\cite{troysegaldiversification}). Diversification is a crucial factor for portfolio optimisation. Portfolio optimisation is defined as selecting the best possible weights for a basket of assets, with a predefined set of objectives such as return maximisation, risk-minimisation, increasing risk-reward ratio, etc (\cite{dariusoh2022mlopt}). The first formal definition of an optimal portfolio was given by \cite{10.2307/2975974}.
The model suggests that an optimal portfolio maximises the expected return for a given variance. \\\\
Let $S= \{s_i \mid i \in \mathbb{W}, 1 \leq i \leq n\}$, where $s_i$ is the value of $i^{th}$ stock. Let $w = \{w_i \mid i \in \mathbb{R^+}\}$, where $w_i$ is the percentage of total capital allocated for the stock $i$, and $\sum_{i=1}^N w_i= 1$. $S$ maps $W$ in a one-to-one fashion, $S:W \rightarrow S$. Let $\mu$ and $\sigma$ denote the expected return and the standard deviation of stock $S$ respectively. Let $R_i$ be the return of the stock $i$. Therefore, the portfolio value is given by\footnote{These equations have been referenced from the book 'Paul Wilmott Introduces Quantitative Finance by \cite{10.5555/1370958}.}
\begin{align}
    \label{eq:portfolio_value}
    V = \sum_{i=0}^N w_i S_i  
\end{align}
After the end of the time horizon, the value of the portfolio becomes
\begin{align}
    \label{eq:portfolio_value_end}
    V + \delta V = \sum_{i=0}^N w_i(1 + R_i)     
\end{align}
We can write the change in the portfolio value as 
\begin{align}
    \label{eq:portfolio_change}
    \frac{\delta V}{V} = \sum_{i=1}^N W_i R_i 
\end{align}
where,
\begin{align}
    \label{eq:updated_weights}
    W_i = \frac{w_i S_i}{\sum_{i=1}^N w_i S_i}
\end{align}
From Equation 4, we can calculate the expected return of the given portfolio
\begin{align}
    \label{eq:expected_return}
    \mu_{V} = \sum_{i=1}^N W_i R_i
\end{align}
and the variance of the return is given by
\begin{align}
    \label{eq:variance_portfolio}
    \sigma_{V}^2 = \sum_{i=1}^N \sum_{j=1}^N W_i W_j \rho_{ij} \sigma_i \sigma_j
\end{align}
Hence, an investor's optimisation problem can be written as
\begin{align*}
 & \phantom{aaa} \max_{\mathbf{w}} \quad \mathbf{w}^T \mathbf{\mu_V} \\
\text{subject to} & \phantom{aaa} \mathbf{w}^T \Sigma \mathbf{w} \leq \sigma^2_{\text{target}}, \\
                   & \phantom{aaa} \mathbf{w}^T \mathbf{1} = 1, \\
                   & \phantom{aaa} \mathbf{w} \geq 0, \\
\text{where}       & \phantom{aaa} \Sigma = \text{Covariance matrix of the assets}.
\end{align*}
The equations mentioned above form the foundation of the Modern Portfolio Theory(MPT), where an investor's problem is to find the portfolio that maximises their expected utility(\cite{10.2307/2975974}). While the mean-variance model has been instrumental in portfolio management, these limitations necessitate advanced models.
\begin{enumerate}
    \item \textbf{Assumption of normal distribution}: MPT follows an assumption that the returns of a particular asset are normally distributed. However, real-life financial data can be skewed and have fat tails (leptokurtic). This drawback can underestimate a portfolio's risk as the model is sensitive to the extreme values of expected returns and variance (\cite{roberto/1331677X.2021.1981963}).
    \item \textbf{Reliance on Historical Data}: MPT relies on historical data to evaluate expected returns and volatility and assumes that these estimates will hold true in the future.. These assumptions may not reflect during unpredictable market conditions. According to the research conducted by \cite{mptlimit1}, MPT's reliance on historical data may result in an incorrect estimation of expected returns, leading to subpar portfolio allocations.
    \item \textbf{Assumption of static correlations}: MPT assumes that the asset returns follow a normal distribution and that the correlations between assets remain constant over time. \cite{9400} argue that MPT's assumptions of normality and constant correlations can lead to a subpar portfolio performance during uncertain market conditions.    
\end{enumerate}
The limitations above can be addressed by using machine learning algorithms. Machine Learning(ML), in general, does not have a single definition and is often context-specific (\cite{kelly2019empirical}). ML is a field of study that involves the development of statistical models that enable computer systems to learn, improve, and improvise from experience without being programmed explicitly. 
Tom Mitchell provides one widely accepted definition of machine learning: "A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E" (\cite{mitchell1997machine}). Machine Learning(ML) algorithms provide several advantages to address the limitations of traditional models. Firstly, unlike MPT, ML models do not necessarily assume to be normally distributed. These algorithms can capture non-linear dependencies and can adapt to data's distribution, generating better accuracy in real-world situations where the data is often skewed or has fat tails (\cite{lipton2018}). Secondly, ML models utilise historical data and adapt to new data, making them more responsive to economic shocks or volatile market conditions (\cite{lee2013}). Consequently, ML-based portfolio optimisation is becoming an increasingly researched field, offering promising results regarding risk-adjusted return and adaptability to market changes. \\\\
Several studies in the field of portfolio management have shown that machine learning algorithms can outperform classic mean-variance optimisation methods. (\cite{banMLPortfolio2018}) introduced performance-based regularisation (PBR), where the main goal was to constrain the sample variances of the expected portfolio risk and return. PBR was employed for both mean-variance and mean-conditional value-at-risk problems. The model displays better benchmarks when compared to L1 and L2 regularisation and the equally weighted portfolio for two out of three  Fama-French data sets.
(\cite{BEHERA2023105843}) performed research where they used a combination of ML models like XGBoost(\cite{Chen_2016}), random forest(\cite{random_forest}), AdaBoost(\cite{freund1995desicion}), Support Vector Machines(SVM) (\cite{cortes1995support}), k-Nearest Neighbours (KNN)(\cite{duda2000pattern}), and Artificial Neural Networks (ANN)\cite{haykin2009neural} to forecast the stocks of the next period. Stocks that yielded greater expected returns were chosen. In the second stage, the mean-value optimiser was employed to assign capital percentages for each stock. Historical data from the Bombay Stock Exchange (BSE), India, Shanghai Stock Exchange, China, and Tokyo Stock Exchange, Japan were used as research samples. The study concluded that the mean-VAR model with AdaBoost outperforms other models. (\cite{BASAK2019552}) performed research where they introduced an experimental framework for the classification problem that predicts the stock market direction, i.e. whether the stock price will increase or decrease, for past prices of $n$ days. Random Forest and Gradient Boosting trees were used for predicting the stock direction. The approach was tested over several companies, where the study achieved high accuracy for medium to long-run prediction of stock direction. (\cite{CHEN2021106943} used a hybrid model of XGBoost and firefly algorithm (\cite{YANG_2}), where XGBoost was used to predict the stock prices of the next period and firefly algorithm to optimise the hyperparameters of the XGBoost. The stocks having high potential returns were selected for portfolio creation. The data used for this study was the Shanghai stock market. The obtained results concluded that the proposed method is better than the traditional way and scores better in benchmarks in terms of risks and returns. Another study done by \cite{Zhang_2020} adopted a method to optimise the Sharpe Ratio directly. Instead of individual assets, they trade Exchange-Traded Funds (ETFs) of market indices to generate a portfolio. The model outperforms several traditional algorithms over the testing period, from 2011 to the end of 2020, including the financial instability during the COVID crisis. \cite{portfolio_construction} investigated the use of random forest classifier to predict the direction of the stock return, after which they used GARCH(\cite{bollerslev1986generalized}) to forecast the stock returns. This dissertation will investigate the use of XGBoost for portfolio optimisation.
\addcontentsline{toc}{section}{Literature Review Summary}
\section*{\large{Literature Review Summary}}
This section provides a generic overview of the existing research relevant to the subject of this dissertation. By presenting a condensed summary of the literature, this section lays the foundation for the subsequent mathematical exploration, enabling a deeper understanding of the research problem and informing the development of the proposed methodology. Figure \ref{fig:generic_overview} serves as the flowchart explaining the sequential steps and logical flow of this section.
\begin{figure}[h]
\centering
\begin{tikzpicture}[
    startstop/.style={rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black, fill=red!30},
    io/.style={trapezium, trapezium left angle=70, trapezium right angle=110, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=blue!30},
    process/.style={rectangle, minimum width=3cm, minimum height=1cm, text centered, text width=3cm, draw=black, fill=orange!30},
    decision/.style={diamond, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=green!30},
    arrow/.style={thick,->,>=stealth},
    node distance=2cm
]

\node (step1) [startstop] {Market Selection};
\node (step2) [process, right of=step1, xshift=2cm] {Feature Engineering};
\node (step3) [process, right of=step2, xshift=2cm] {Predicting Stock Direction};
\node (step4) [process, right of=step3, xshift=2cm] {Forecast Returns};
\node (step5) [process, right of=step4, xshift=2cm] {Best Stock Selection};
\node (step6) [process, below of=step5, yshift=-2cm] {Optimal Weight Allocation};
\node (step7) [process, left of=step6, xshift=-2cm] {Back Testing};
\node (step8) [startstop, left of=step7, xshift=-2cm] {Evaluate Results};

\draw [arrow] (step1) -- (step2);
\draw [arrow] (step2) -- (step3);
\draw [arrow] (step3) -- (step4);
\draw [arrow] (step4) -- (step5);
\draw [arrow] (step5) -- (step6);
\draw [arrow] (step6) -- (step7);
\draw [arrow] (step7) -- (step8);

\end{tikzpicture}
\caption{Generic overview of MLbased portfolio optimisation}
\label{fig:generic_overview}
\end{figure}
\newpage
\noindent
The flow diagram above outlines an eight-step approach to stock market investment leveraging machine learning algorithms.
\begin{enumerate}
    \item \textbf{Market Selection}: The first step involves choosing the relevant market for analysis. This step acts as the foundation for the rest of the processes.
    \item \textbf{Feature Engineering}: Appropriate features are generated for ML algorithms. This step acts as the backbone of the predictive models.
    \item \textbf{Predicting Stock Direction}: ML algorithms are utilised to predict the stock direction, i.e. whether the price increases or decreases concerning the current price.
    \item \textbf{Forecast Returns}: After predicting stock direction, the model then forecasts the returns for a time frame using a time-series algorithm.
    \item \textbf{Best Stock Selection}: After evaluating the forecasts, stocks projected to have the highest expected returns are selected for investment.
    \item \textbf{Optimal Weight Allocation}: The data from the selected stocks are fed into the mean-variance model, which generates optimal weights for capital allocation.
    \item \textbf{Back Testing}: Prior implementation, the strategy is usually back-tested on unseen data, thus validating the model's efficacy.
    \item \textbf{Evaluate Results}: Finally, the results from the back-testing are evaluated to make the final investment decisions.
\end{enumerate}
\addcontentsline{toc}{section}{XGBoost}
\newpage
\section*{\large{XGBoost}\footnote{The content in this subsection draws extensively on the research presented in 'XGBoost: A Scalable Tree Boosting System,' a paper by Chen and Guestrin published in the proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 785-794 \citep{Chen_2016}}}
\addcontentsline{toc}{subsection}{Introduction to XGBoost and similar literature}
\subsection*{\small{Introduction to XGBoost and similar literature}}
XGBoost, short for Extreme Gradient Boosting, is a powerful ML algorithm that has gained significant popularity among machine learning practitioners due to its remarkable success in various predictive modelling tasks. It is an advancement of gradient boosting, a technique that utilises multiple weak models to create a robust ensemble model. Here are some key features of the XGBoost Model.
\begin{enumerate}
    \item \textbf{Gradient Boosting}: XGBoost utilises the gradient boosting decision tree algorithm to construct new trees that predict and correct the residuals, reducing the misclassification rate.
    \item \textbf{Regularisation}: XGBoost has a regularisation term in its objective function to control the overall complexity of the model, reducing overfitting.
    \item \textbf{Null value awareness}: XGBoost utilises a sparsity-aware algorithm to handle sparse and missing data points. It automatically learns the best direction to handle the missing values.
    \item \textbf{Parallel Processing}: XGBoost is designed to perform \href{https://www.techtarget.com/searchdatacenter/definition/parallel-processing}{parallel processing}. This feature makes computation faster than other boosting algorithms.
    \item \textbf{Flexibility}: XGBoost algorithm allows users to define their custom optimisation objectives and evaluation criteria, adding flexibility for the user.
    \item \textbf{Continued Training}: The practitioner can start training an XGBoost model from its previous iteration, allowing them to tweak the portfolio weights when real-time dynamic data is incorporated from the stock market.
\end{enumerate}
Presently, there are many studies conducted where researchers utilised the XGBoost algorithm extensively in the domain of portfolio optimisation. A study conducted by (\cite{DEZHKAM2023105626}) used the combination of Hilbert-Huang Transformation (\href{https://en.wikipedia.org/wiki/Hilbert\%E2\%80\%93Huang_transform}{HHT}) and XGBoost to perform feature engineering and close price trend classification respectively. The classification outputs were used to balance the portfolio weights based on the trend. The performance of the portfolios under the combination of HHT and XGBoost performed 99.8\% better than the portfolio utilising raw financial data (\cite{DEZHKAM2023105626}). Another research conducted by (\cite{Wang2022}) used XGBoost to select the best stocks. Factors such as trade volume, the adjusted closing price of three days, and the difference between high and low prices were taken as features, with the highest importance given to the adjusted closing price of three days. The evaluation metrics used for tuning the model were \href{https://en.wikipedia.org/wiki/Root-mean-square_deviation}{RMSE} and \href{https://en.wikipedia.org/wiki/Mean_absolute_percentage_error}{MAPE}. The selected stock returns were fed into the mean-variance model to optimise the Sharpe ratio (\cite{Wang2022}).
\addcontentsline{toc}{subsection}{Mathematics behind XGBoost}
\subsection*{\small{Mathematics behind XGBoost}}
Before delving into the mathematical aspects, here are the notations of this section that are pertinent to the mathematical framework of XGBoost.
\begin{enumerate}
    \item \textbf{Tree}: A tree refers to a decision tree, mainly used to generate predictive models. It is a hierarchical structure of nodes and branches that represents a series of binary decisions before making the final decision.
    \item \textbf{Node}: A node is a particular point in a tree where a binary decision is made based on a specific feature and its corresponding threshold.
    \item \textbf{Leaf}: A leaf is the terminal node of a decision tree, where a specific outcome or class label is generated based on the outcomes of the preceding nodes.
    \item \textbf{Loss function}: A loss function, also known as cost function, is a method of evaluating how well an algorithm models the data. 
    \item \textbf{Gradient}: Gradient refers to a partial derivative of a loss function with respect to the predicted values. It represents the direction and the magnitude of the descent in the context of the loss function.
    \item \textbf{Hessian}: Hessian refers to the second partial derivative of the loss function with respect to the predicted values. 
    \item \textbf{Ensemble}: An ensemble method combines the outputs from multiple models to improve the overall performance and accuracy, reducing the possibility of poor selection due to individual model biases or variance.
        
\end{enumerate}
Let $\mathcal{D}$ be the given dataset with $m$ examples and $n$ features, each example represented as a pair $(x_i, y_i)$, where $x_i \in \mathrm{R^n}$ and $y_i \in \mathrm{R}$. A tree ensemble model uses K additive functions to predict the output.
\begin{align}
    \label{eq:prediction_xgb}
    \hat{y} = \phi(x_i) = \sum_{k=1}^K f_k\left(x_i\right), \quad f_k \in \mathcal{F}
\end{align}
where $\mathcal{F} = \{f(x) = w_{q(x)}\}$, $(q: \mathrm{R^m} \rightarrow T, \quad w \in \mathrm{R^T})$ is the \href{https://en.wikipedia.org/wiki/\%CE\%A3-algebra}{sigma-algebra} of the regression trees, also known as \href{https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/}{CART}. Here, $q$ represents the structure of each tree that maps to the corresponding index of the leaf. Here, T is the number of leaves. Each $f_k$  maps an independent tree structure $q$ and weights $w$. \\
To learn the set of functions fed to the model, the main objective becomes the minimisation of the following function (regularisation objective).
\begin{align}
    \label{eq: regularisation_function}
    \mathcal{L}\left(\phi\right) = \underbrace{\sum_i L\left(\hat{y_i}, y_i\right)}_{\text{Residual error}} + \underbrace{\sum_k \Omega \left(f_k\right)}_{\text{Cost function}}
    \qquad
\end{align}
where,
\begin{align}
    \label{eq:cost_function}
    \Omega\left(f\right) = \gamma T + \frac{1}{2} \lambda \|w\|^2
\end{align}
Here, $L$ is a continuous-differentiable convex loss function that evaluates the difference between the predicted value $\hat{y_i}$ and the actual value $y_i$. The term $\Omega$ is used to penalise the complexity of the model. The additional regularisation term is used to smooth the final learnt weights to avoid overfitting. $\gamma$ is the minimum loss reduction required to make a further partition on a leaf node of the tree. It is used to prune unnecessary splits. $\lambda$ is used as for \href{https://developers.google.com/machine-learning/crash-course/regularization-for-simplicity/l2-regularization}{L2} regularisation metric on the weights of the tree. \\\\
The tree ensemble model presented in equations \ref{eq: regularisation_function} and \ref{eq:cost_function} include functions as parameters that cannot be optimised using the traditional optimisation methods in the Euclidean space. Since XGBoost is a tree-based model, the parameters are not continuous\footnote{In the context of decision tree models, parameters not considered as continuous imply that small changes in these parameters do not lead to small changes in the model's output.} and traditional converging optimisation methods cannot be used. To overcome this challenge, gradient boosting is used. XGBoost uses gradient boosting to train the individual trees iteratively, where each new tree tries to correct the mistakes of the previous ones. \\\\
Let $\hat{y}^{(t)}_i$ be the $i^{th}$ instance of the $t^{th}$ iteration and using $f_t$ to minimise the following objective.
\begin{align}
    \label{eq:GB_optimisation}
    \mathcal{L}^{(t)} = \sum_{i=1}^n L\left(y_i, \hat{y}^{(t-1)}_i + f_t \left(x_i\right)\right) + \Omega \left(f_t\right)
\end{align}
Here, $f_t$ acts as a greedy parameter that improves the algorithm according to the equations \ref{eq: regularisation_function} and \ref{eq:cost_function}. Performing second-order approximation of equation \ref{eq:GB_optimisation} leads to
\begin{align}
    \label{eq:second_order_approx}
    \mathcal{L}^{\left(t\right)} &\approx \sum_{i=1}^n \left[\underbrace{L(y_i, \hat{y}^{(t-1)}_i)}_{\text{constant}} + {g_i f_t(x_i)} + \frac{1}{2} h_i f_t^2(x_i)\right] + \Omega (f_t)
\end{align}
where,
\begin{align}
    \label{eq:gradient}
    g_i = \frac{\partial}{\partial \hat{y}^{(t-1)}}{L(y_i, \hat{y}^{(t-1)}_i)}
\end{align}
\begin{align}
    \label{eq:hessian}
    h_i =  \frac{\partial}{\partial \hat{y}^{(t-1)}} g_i
\end{align}
The equations \ref{eq:gradient} and \ref{eq:hessian} are the first derivative(gradient) and second derivative(hessian) of the cost function respectively. We remove the constant\footnote{The term $L(y_i, \hat{y_i}^{t-1})$ is considered "constant" not because its value doesn't change, but because its value does not depend on the new tree.} terms to obtain the following equation
\begin{align}
    \label{eq:simplified_obj_fucntion}
    \Tilde{\mathcal{L}}^{(t)} = \sum_{i=1}^n \left[{g_i f_t(x_i)} + \frac{1}{2} h_i f_t^2(x_i)\right] + \Omega (f_t)
\end{align}
Let $I_j = \{i|q(x_i) = j\}$ be an instance of leaf $j$. Equation \ref{eq:simplified_obj_fucntion} after expanding $\Omega$ can be written as
\begin{align}
    \label{eq:updated_cost_function}
    \Tilde{\mathcal{L}}^{(t)} = \sum_{j=1}^{T}\left[(\sum_{i \in I_j} g_i)w_j + \frac{1}{2}(\sum_{i \in I_j} h_i + \lambda)w_j^2\right] + \gamma T
\end{align}
The optimal weight is derived by minimizing the equation \ref{eq:updated_cost_function} w.r.t. $w_j$. The optimal weight ($w_j^*$) of the leaf $j$  can be computed as
\begin{gather*}
    \frac{\partial}{\partial w_j} \Tilde{\mathcal{L}}^{(t)} = 0 \\
    \implies \frac{\partial}{\partial w_j} \left\{\sum_{j=1}^{T}\left[(\sum_{i \in I_j} g_i)w_j + \frac{1}{2}(\sum_{i \in I_j} h_i + \lambda)w_j^2\right] + \gamma T\right\} = 0 \\\\
    \implies (\sum_{i \in I_j} g_i) + (\sum_{i \in I_j} h_i + \lambda)w_j^* = 0 
\end{gather*}
\begin{align}
    \label{eq: Optimal weight}
    \therefore w_j^* = - \frac{\sum_{i \in I_j} g_i}{\sum_{i \in I_j} h_i + \lambda}
\end{align}
and the optimal value is calculated by substituting equation \ref{eq: Optimal weight} in equation \ref{eq:updated_cost_function}
\begin{align}
    \label{eq: optimal value}
    \Tilde{\mathcal{L}}^{(t)}(q) = - \frac{1}{2} \sum_{j=1}^{T} \frac{(\sum_{i \in I_j g_i})^2}{\sum_{i \in I_j} + \lambda} + \gamma T 
\end{align}
Equation \ref{eq: optimal value} can be utilised as an evaluation function to measure the quality of the tree structure $q$. This score is similar to the impurity score used to evaluate decision trees, except that it is derived for a broader range of objective functions\footnote{I would like to express my gratitude to Josh Staramer for his valuable explanations of the XGBoost algorithm in his \href{https://www.youtube.com/@statquest}{StatQuest} YouTube channel. His videos have greatly helped me in understanding and explaining this algorithm.
}.
